## Output Indicators

- ğŸ” Scraping in progress
- âœ… Successful crawl
- âŒ Failed to process
- ğŸ“„ Internal page queued
- ğŸ”— External link queued
- ğŸŒ Validating external link
- â­ï¸ Skipping already visited URL


## Implementation Details

- Uses Go's `sync.WaitGroup` for worker coordination
- Utilizes `golang.org/x/net/html` for HTML parsing
- Handles relative and absolute URLs using Go's `net/url` package

## Error Handling

The crawler includes robust error handling for:
- Invalid URLs
- Network failures
- HTTP errors
- Malformed HTML
- External link validation


## Project Directory Structure

```plaintext
webscraper/
â”œâ”€â”€ cmd/                    # Entry points for CLI/web scraper
â”‚   â”œâ”€â”€ webscraper/
â”‚   â”‚   â”œâ”€â”€ main.go         # Main function
â”œâ”€â”€ internal/               # Core business logic
â”‚   â”œâ”€â”€ crawler/            # Web crawling logic
â”‚   â”‚   â”œâ”€â”€ crawler.go      # Main recursive crawling logic
â”‚   â”œâ”€â”€ utils/              # Stores visited URLs
â”‚   â”‚   â”œâ”€â”€ url_utils.go    # Manages visited links
â”‚   â”œâ”€â”€ validator/          # Link validation
â”‚   â”‚   â”œâ”€â”€ link_validator.go    # Checks for valid links
â”œâ”€â”€ go.mod                  # Go module file
â”œâ”€â”€ go.sum                  # Dependencies
â”œâ”€â”€ README.md               # Documentation
