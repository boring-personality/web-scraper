
## Configuration

The crawler accepts several parameters:
- `client`: An HTTP client for making requests
- `startURL`: The initial URL to begin crawling from
- `numScrapers`: Number of concurrent scraping workers
- `numValidators`: Number of concurrent external link validators

## Output Indicators

- ğŸ” Scraping in progress
- âœ… Successful crawl
- âŒ Failed to process
- ğŸ“„ Internal page queued
- ğŸ”— External link queued
- ğŸŒ Validating external link
- â­ï¸ Skipping already visited URL
- ğŸ“Š Final crawl statistics

## Implementation Details

- Uses Go's `sync.WaitGroup` for worker coordination
- Implements thread-safe operations with `sync.RWMutex`
- Employs channels for concurrent communication
- Utilizes `golang.org/x/net/html` for HTML parsing
- Handles relative and absolute URLs using Go's `net/url` package

## Error Handling

The crawler includes robust error handling for:
- Invalid URLs
- Network failures
- HTTP errors
- Malformed HTML
- External link validation


webscraper/
â”‚â”€â”€ cmd/                    # Entry points for CLI/web scraper
â”‚   â”œâ”€â”€ webscraper/
â”‚   â”‚   â”œâ”€â”€ main.go         # Main function
â”‚â”€â”€ internal/               # Core business logic
â”‚   â”œâ”€â”€ crawler/            # Web crawling logic
â”‚   â”‚   â”œâ”€â”€ crawler.go      # Main recursive crawling logic
â”‚   â”œâ”€â”€ utils/            # Stores visited URLs
â”‚   â”‚   â”œâ”€â”€ url_utils.go      # Manages visited links
â”‚   â”œâ”€â”€ validator/         # Link validation
â”‚   â”‚   â”œâ”€â”€ link_validator.go    # Checks for valid links
â”‚â”€â”€ go.mod                  # Go module file
â”‚â”€â”€ go.sum                  # Dependencies
â”‚â”€â”€ README.md               # Documentation
